{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHVM1RjJkV0hBgDCWO1krg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushm/SRIP-IITGN-2022/blob/main/Neural%20Network%20Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "Neural networks consists of neurons, connections between these neurons called weights and some biases connected to each neuron. We distinguish between input, hidden and output layers, where we hope each layer helps us towards solving our problem.\n",
        "\n",
        "It is designed to recognize patterns in complex data, and often performs the best when recognizing patterns in audio, images or video."
      ],
      "metadata": {
        "id": "2ozrskC7zBbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the required jax numpy library for calculations\n",
        "import jax.tools.colab_tpu\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import jax\n",
        "from jax import jit, vmap, pmap, grad, value_and_grad, random\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# importing matplotlib for visualization\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BjEeO5o3P7I1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "NvCrdKWplW8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [784, 512, 512, 10]\n",
        "step_size = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "n_targets = 10\n",
        "mnist_img_size = (28, 28)\n",
        "\n",
        "seed = 0\n",
        "key = jax.random.PRNGKey(seed)"
      ],
      "metadata": {
        "id": "3NW2-jOelRZc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(layer_sizes, key, step_size=step_size):\n",
        "  params = []\n",
        "  keys = random.split(key, num=len(layer_sizes)-1)\n",
        "  \n",
        "  # loop to randomly initialize weights and biases\n",
        "  # for a dense neural network layer\n",
        "  for in_width, out_width, key in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
        "    weight_key, bias_key = jax.random.split(key) # Bias is used to approximate where the value of the new neuron starts to be meaningful\n",
        "    params.append([\n",
        "          step_size * jax.random.normal(weight_key, shape=(out_width, in_width)),\n",
        "          step_size * jax.random.normal(bias_key, shape=(out_width,))\n",
        "          ])\n",
        "  return params\n",
        "\n",
        "\n",
        "params = init_network_params(layer_sizes, key, step_size)\n",
        "print(jax.tree_map(lambda x: x.shape, params))"
      ],
      "metadata": {
        "id": "-bFD8iERrxHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84cc266-9714-462d-8845-931820494f60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(512, 784), (512,)], [(512, 512), (512,)], [(10, 512), (10,)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward pass\n",
        "To move forward through the network, called a forward pass, we iteratively use a formula to calculate each neuron in the next layer.\n",
        "\n",
        "We have something called mini-batches(batched_predict), where we average the gradient of some number of defined observation per mini-batch, and then we have the basic neural network setup."
      ],
      "metadata": {
        "id": "-FM_oSJAzget"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function for the neurons\n",
        "# ReLU stands for Rectified Linear Unit.\n",
        "# The main advantage is simple and sparsity\n",
        "def relu(x):\n",
        "  return jnp.maximum(0, x)\n",
        "\n",
        "\n",
        "# predict function produces result of the network prediction for each sample\n",
        "# This function is very similar to feed forward,\n",
        "# First we find neurons' values for hidden layer, then for output layer\n",
        "def predict(params, image):\n",
        "   hidden_layers = params[:-1]\n",
        "\n",
        "   activations = image\n",
        "   for w, b in hidden_layers:\n",
        "     outputs = jnp.dot(w, activations) + b\n",
        "     activations = relu(outputs)\n",
        "\n",
        "   final_w, final_b = params[-1]\n",
        "   logits = jnp.dot(final_w, activations) + final_b\n",
        "   return logits - logsumexp(logits) \n",
        "\n",
        "\n",
        "x = np.random.randn(np.prod(mnist_img_size))\n",
        "random_flattened_images = np.random.randn(16, np.prod(mnist_img_size))\n",
        "\n",
        "# use JAX’s vmap function to automatically handle mini-batches, \n",
        "# with no performance penalty\n",
        "batched_predict = vmap(predict, in_axes=(None,0))\n",
        "predictions = batched_predict(params, random_flattened_images)\n",
        "print(predictions.shape)"
      ],
      "metadata": {
        "id": "D-Xr_EfIP53l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6072816-19c5-4ad0-9a92-948c8cba99c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have all the ingredients we need to define our neural network and train it. "
      ],
      "metadata": {
        "id": "YWqfEST1pqYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "Trying to classify digits from 0 - 9 using a dataset called MNIST. This data set consists of 70,000 images that are 28 by 28 pixels each. The data set contains one label for each image that specifies the digit and there are 10 classes/labels.\n"
      ],
      "metadata": {
        "id": "r7mh-Acfsx81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_transform(x):\n",
        "    return jnp.ravel(jnp.array(x, dtype=jnp.float32))\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = jnp.array(transposed_data[1])\n",
        "    imgs = jnp.stack(transposed_data[0])\n",
        "\n",
        "    return imgs, labels\n",
        "\n",
        "\n",
        "train_dataset = MNIST(root='train_mnist', train=True, download=True, transform=custom_transform)\n",
        "test_dataset = MNIST(root='test_mnist', train=False, download=True, transform=custom_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "batch_data = next(iter(train_loader))\n",
        "imgs = batch_data[0]\n",
        "lbls = batch_data[1]\n",
        "print(imgs.shape, imgs[0].dtype, lbls.shape, lbls[0].dtype)\n",
        "\n",
        "# optimization - loading the whole dataset into memory\n",
        "train_images = jnp.array(train_dataset.data).reshape(len(train_dataset), -1)\n",
        "train_lbls = jnp.array(train_dataset.targets)\n",
        "\n",
        "test_images = jnp.array(test_dataset.data).reshape(len(test_dataset), -1)\n",
        "test_lbls = jnp.array(test_dataset.targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q172TJ7UYCkb",
        "outputId": "1c889a8b-4198-4bc5-8549-5f2b3cb9a390"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 784) float32 (128,) int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Utility, loss functions\n",
        "Defining the layers in an exact way. \n",
        "1. **Input layer**: In this layer, I input my data set consisting of 28x28 images and flatten these images into one array with 28×28=78428×28=784 elements. This means that the input layer will have 784 nodes.\n",
        "\n",
        "2. Hidden layer 1: In this layer, I reduce the number of nodes from 784 in the input layer to 512 nodes.\n",
        "\n",
        "3. Hidden layer 2: In this layer, I decide to go with 256 nodes, from the 512 nodes in the first hidden layer. This is no new challenge because I've already reduced the number in the first layer.\n",
        "\n",
        "4. Output layer: In this layer, I reduce the 256 nodes to a total of 10 nodes so that I can evaluate the nodes against the label. This label is received in the form of an array with 10 elements, where one of the elements is 1 while the rest are 0.\n",
        "\n",
        "The specific number of nodes chosen were at random, although decreasing to avoid overfitting."
      ],
      "metadata": {
        "id": "VRh3gI9ysiuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(params, images, gt_lbls):\n",
        "    predictions = batched_predict(params, images)\n",
        "    return -jnp.mean(predictions * gt_lbls)\n",
        "\n",
        "\n",
        "def accuracy(params, images, target_class):\n",
        "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
        "    return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "# use jit to speed up everything\n",
        "@jit\n",
        "def update(params, images, gt_lbls, lr=step_size):\n",
        "    # use grad to take the derivative of the loss \n",
        "    # with respect to the neural network parameters\n",
        "    loss_value, grads = value_and_grad(loss)(params, images, gt_lbls)\n",
        "    return loss_value, jax.tree_multimap(lambda p, g: p - lr*g, params, grads)\n",
        "\n",
        "# Training\n",
        "# Passing in an array of sizes that defines the number of activations for each layer and key\n",
        "params = init_network_params([np.prod(mnist_img_size), 512, 256, len(MNIST.classes)], key, step_size)\n",
        "\n",
        "loss_values = []\n",
        "# There are two main loops in the training function. \n",
        "# One loop for the number of epochs, which is the number of times to run the entire data set,\n",
        "# and a second loop for running through each observation one by one.\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "    for cnt, (imgs, lbls) in enumerate(train_loader):\n",
        "        gt_labels = jax.nn.one_hot(lbls, len(MNIST.classes))\n",
        "        loss, params = update(params, imgs, gt_labels)    \n",
        "        losses.append(loss)\n",
        "    loss_values.append(jnp.average(losses))\n",
        "    print(f'Epoch {epoch}, train acc = {accuracy(params, train_images, train_lbls)} test acc = {accuracy(params, test_images, test_lbls)} loss = {loss_values[epoch]}')\n",
        "plt.plot(loss_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "nYHE0J8VduRl",
        "outputId": "84c9fd06-004b-41e6-c151-33ef61b999f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train acc = 0.9142166972160339 test acc = 0.9169999957084656 loss = 0.054982125759124756\n",
            "Epoch 1, train acc = 0.9355666637420654 test acc = 0.9348999857902527 loss = 0.026666440069675446\n",
            "Epoch 2, train acc = 0.9470333456993103 test acc = 0.9444999694824219 loss = 0.021401291713118553\n",
            "Epoch 3, train acc = 0.9523833394050598 test acc = 0.9501000046730042 loss = 0.018027668818831444\n",
            "Epoch 4, train acc = 0.9585666656494141 test acc = 0.9532999992370605 loss = 0.015658993273973465\n",
            "Epoch 5, train acc = 0.9633833169937134 test acc = 0.9589999914169312 loss = 0.013738561421632767\n",
            "Epoch 6, train acc = 0.9694333672523499 test acc = 0.9627999663352966 loss = 0.012361864559352398\n",
            "Epoch 7, train acc = 0.9718000292778015 test acc = 0.9643999934196472 loss = 0.011097724549472332\n",
            "Epoch 8, train acc = 0.9749333262443542 test acc = 0.9670999646186829 loss = 0.010092552751302719\n",
            "Epoch 9, train acc = 0.9764666557312012 test acc = 0.967199981212616 loss = 0.009215205907821655\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "1nGgx4rcF69n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, lbls = next(iter(test_loader))\n",
        "img = imgs[9].reshape(mnist_img_size)\n",
        "gt_lbl = lbls[9]\n",
        "print(img.shape)\n",
        "\n",
        "\n",
        "pred = jnp.argmax(predict(params, np.ravel(img)))\n",
        "print('prediction: ', pred)\n",
        "print('actual: ', gt_lbl)\n",
        "\n",
        "plt.imshow(img); plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "KiUuIQhVY5Y2",
        "outputId": "488bcf00-9bb2-43c2-deb0-f1b0a4f903c2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "prediction:  9\n",
            "actual:  9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "We’ve used the whole of the JAX API: grad for derivatives, jit for speedups and vmap for auto-vectorization. We used NumPy to specify all of our computation, and borrowed the great data loaders from torch/datasets."
      ],
      "metadata": {
        "id": "Rxn-CrLttGwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important thing to remember is that the neural network is simply trained to reduce output error by modifying weights and biases. Feedforward and backpropagation are used in the training process. Backpropagation adjusts weights and biases to reduce the output error, i.e. the difference between predicted and true values, whereas feedforward predicts output."
      ],
      "metadata": {
        "id": "vKXNzz8_76tK"
      }
    }
  ]
}